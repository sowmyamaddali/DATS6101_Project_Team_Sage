# Have not found a good way to display it.
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(admitLogit, type = "response" )
Admit$prob=prob
h <- roc(admit~prob, data=Admit)
auc(h) # area-under-curve prefer 0.8 or higher.
plot(h)
# unloadPkg("pROC")
unloadPkg("pROC")
admitNullLogit <- glm(admit ~ 1, data = Admit, family = "binomial")
mcFadden = 1 - logLik(admitLogit)/logLik(admitNullLogit)
mcFadden
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
admitLogitpr2 = pR2(admitLogit)
admitLogitpr2
unloadPkg("pscl")
loadPkg("aod")  # Analysis of Overdispersed Data, used wald.test in logit example
wald.test(b = coef(admitLogit), Sigma = vcov(admitLogit), Terms = 4:6)
l <- cbind(0, 0, 0, 1, -1, 0)
wald.test(b = coef(admitLogit), Sigma = vcov(admitLogit), L = l)
## odds ratios and 95% CI
exp(cbind(OR = coef(admitLogit), confint(admitLogit)))
newdata1 <- with(Admit, data.frame(gre = mean(gre), gpa = mean(gpa), rank = factor(1:4)))
## view data frame
newdata1
newdata1$rankP <- predict(admitLogit, newdata = newdata1, type = "response")
# type = c("link","response", "terms") # returns the link function value (the log-odds), the response variable (probability p), and the individual linear term combo respectively.
newdata1
newdata2 <- with(Admit, data.frame(gre = rep(seq(from = 200, to = 800, length.out = 100), 4), gpa = mean(gpa), rank = factor(rep(1:4, each = 100))))
newdata3 <- cbind(newdata2, predict(admitLogit, newdata = newdata2, type = "link", se = TRUE))
newdata3 <- within(newdata3, {
PredictedProb <- plogis(fit)
LL <- plogis(fit - (1.96 * se.fit))
UL <- plogis(fit + (1.96 * se.fit))
})
## view first few rows of final dataset
head(newdata3)
ggplot(newdata3, aes(x = gre, y = PredictedProb)) + geom_ribbon(aes(ymin = LL,
ymax = UL, fill = rank), alpha = 0.2) + geom_line(aes(colour = rank),
size = 1)
# bikeorig <- read.csv("bikedata.csv")
bikeorig <- data.frame(read.csv("bikedata.csv")) # make no difference here
# head(bikeorig)
str(bikeorig)
bikepeak <- subset(bikeorig,subset=(bikeorig$Hour >14 & bikeorig$Hour < 18)) # only the three peak hours
colnames(bikepeak)[5:14] <- c("day","workday","weather","tempF","tempFF","humidity","wind","cusers","rusers","tusers") # rename some columns
bikeclean = bikepeak[ , c("Season", "day", "workday", "weather", "tempFF", "humidity", "wind", "tusers")] # only take eight columns Hour, day, workday, weather, tempFF, humidity, wind, tusers
bikeclean$y = bikepeak[,4] # copy Holiday column and call it 'y'
#convert some columns into factos as appropriate
bikeclean$Season = factor(bikeclean$Season)
bikeclean$day = factor(bikeclean$day)
bikeclean$workday = factor(bikeclean$workday)
bikeclean$weather = factor(bikeclean$weather)
bikeclean$y <- ifelse(bikeclean$y == 1,TRUE,FALSE)
bikeclean$y = factor(bikeclean$y)
str(bikeclean)
loadPkg("leaps")
reg.leaps <- regsubsets(y~., data = bikeclean, nbest = 1, method = "exhaustive")  # leaps,
plot(reg.leaps, scale = "adjr2", main = "Adjusted R^2")
plot(reg.leaps, scale = "bic", main = "BIC")
plot(reg.leaps, scale = "Cp", main = "Cp")
loadPkg("bestglm")
res.bestglm <- bestglm(Xy = bikeclean, family = binomial,
IC = "AIC",                 # Information criteria for
method = "exhaustive")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm")
unloadPkg("leaps") # leaps is required by bestglm, thus cannot be unloaded before bestglm
unloadPkg("pROC")
unloadPkg("aod")
library(ggplot2)
library(gridExtra)
library(corrplot)
library(ROSE)
library(dplyr)
library(smotefamily)
library(caTools)
library(randomForest)
library(rpart)
library(rpart.plot)
library("pROC")
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("FNN")
#For this example we are going to use the IRIS dataset in R
str(iris)
loadPkg('ggplot2')
#plot(iris[,1],iris[,2], col=iris$Species)
for (xx in 1:(length(iris)-2) ) {
for (yy in (xx+1):(length(iris)-1) ) {
print(xx)
print(yy)
p <- ggplot(iris, aes(x=iris[,xx], y=iris[,yy], color=Species)) +
geom_point() +
scale_color_manual(values = c("setosa" = "purple", "versicolor"="orange","virginica"="steelblue")) +
labs( x = colnames(iris)[xx], y = colnames(iris)[yy], title = paste(colnames(iris)[yy],"vs",colnames(iris)[xx]) )
print(p)
}
}
# xx <- 1
# yy <- 2
# ggplot(iris, aes(x=iris[,xx], y=iris[,yy], color=Species)) +
#   geom_point() +
#   scale_color_manual(values = c("setosa" = "purple", "versicolor"="orange","virginica"="steelblue")) +
#   labs( x = colnames(iris)[xx], y = colnames(iris)[yy], title = paste(colnames(iris)[yy],"vs",colnames(iris)[xx]) )
#first we want to scale the data so KNN will operate correctly
scalediris <- as.data.frame(scale(iris[1:4], center = TRUE, scale = TRUE))
#We also need to create test and train data sets, we will do this slightly differently by using the sample function. The 2 says create 2 data sets essentially, replacement means we can reset the random sampling across each vector and the probability gives sample the weight of the splits, 2/3 for train, 1/3 for test.
set.seed(1000)
iris_sample <- sample(2, nrow(scalediris), replace=TRUE, prob=c(0.67, 0.33))
#We then just need to use the new variable to create the test/train outputs, selecting the first four rows as they are the numeric data in the iris data set and we want to predict Species
iris_training <- scalediris[iris_sample==1, 1:4]
iris_test <- scalediris[iris_sample==2, 1:4]
#Now we need to create our 'Y' variables or labels need to input into the KNN function
#Fifth element is species
iris.trainLabels <- iris[iris_sample==1, 5]
iris.testLabels <- iris[iris_sample==2, 5]
#So now we will deploy our model
iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=7)
#iris_pred
loadPkg("gmodels")
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
#Looks like we got all but three correct, not bad
#The second no is recall and the third is precision, the last one is just a percentage
loadPkg("gmodels")
loadPkg("FNN")
loadPkg("caret") # confusionMatrix
# Loop thru different k values
# create an empty dataframe to store the results from confusion matrices
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
for (kval in 3:11) {
iris_pred <- knn(train = iris_training, test = iris_test, cl=iris.trainLabels, k=kval)
IRISPREDCross <- CrossTable(iris.testLabels, iris_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
IRISPREDCross
#
cm = confusionMatrix(iris_pred, reference = iris.testLabels ) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, "Total Accuracy Summary")
bank_data = read.csv("bank.csv",                #<- name of the data set.
check.names = FALSE,       #<- don't change column names.
stringsAsFactors = FALSE)  #<- don't convert the numbers and characters to factors.
# Check the structure and view the data.
str(bank_data)
xkabledplyhead(bank_data)
xkabledplytail(bank_data)
# Let's run the kNN algorithm on our banking data.
# Check the composition of labels in the data set.
table(bank_data$`signed up`)
table(bank_data$`signed up`)[2] / sum(table(bank_data$`signed up`))
# This means that at random, we have an 11.6% chance of correctly picking
# out a subscribed individual. Let's see if kNN can do any better.
# Let's split the data into a training and a test set.
# Sample 80% of our know data as training and 20% as test.
set.seed(1)
bank_data_train_rows = sample(1:nrow(bank_data),     #<- from 1 to the number of rows in the data set
round(0.8 * nrow(bank_data), 0),  #<- multiply the number of rows by 0.8 and round the decimals
replace = FALSE)       #<- don't replace the numbers
# Let's check to make sure we have 80% of the rows.
length(bank_data_train_rows) / nrow(bank_data)
bank_data_train = bank_data[bank_data_train_rows, ]  #<- select the rows identified in the bank_data_train_rows data
bank_data_test = bank_data[-bank_data_train_rows, ]  #<- select the rows that weren't identified in the bank_data_train_rows data
# Check the number of rows in each set.
nrow(bank_data_train)
nrow(bank_data_test)
# Let's train the classifier for k = 3.
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class")
loadPkg("class")
# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
bank_3NN = knn(train = bank_data_train[, c("age", "balance", "duration")],  #<- training set cases
test = bank_data_test[, c("age", "balance", "duration")],    #<- test set cases
cl = bank_data_train[, "signed up"],                         #<- category for true classification
k = 3) #,                                                    #<- number of neighbors considered
# use.all = TRUE)                                            #<- control ties between class assignments
#   If true, all distances equal to the k-th largest are included
# View the output.
str(bank_3NN)
length(bank_3NN)
table(bank_3NN)
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the
# predictions from bank_3NN to the original data set.
kNN_res = table(bank_3NN,
bank_data_test$`signed up`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples
# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]
# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
loadPkg("caret")
cm = confusionMatrix(bank_3NN, reference = as.factor(bank_data_test[, "signed up"]) )
cm$overall
cm$byClass
chooseK = function(k, train_set, val_set, train_class, val_class){
# Build knn with k neighbors considered.
set.seed(1)
class_knn = knn(train = train_set,    #<- training set cases
test = val_set,       #<- test set cases
cl = train_class,     #<- category for classification
k = k) #,                #<- number of neighbors considered
# use.all = TRUE)       #<- control ties between class assignments. If true, all distances equal to the k-th largest are included
tab = table(class_knn, val_class)
# Calculate the accuracy.
accu = sum(tab[row(tab) == col(tab)]) / sum(tab)
cbind(k = k, accuracy = accu)
}
# The sapply() function plugs in several values into our chooseK function.
# function(x)[function] allows you to apply a series of numbers
# to a function without running a for() loop.
knn_different_k = sapply(seq(1, 21, by = 2),  #<- set k to be odd number from 1 to 21
function(x) chooseK(x,
train_set = bank_data_train[, c("age", "balance", "duration")],
val_set = bank_data_test[, c("age", "balance", "duration")],
train_class = bank_data_train[, "signed up"],
val_class = bank_data_test[, "signed up"]))
# Reformat the results to graph the results.
str(knn_different_k)
knn_different_k = data.frame(k = knn_different_k[1,],
accuracy = knn_different_k[2,])
# Plot accuracy vs. k.
# install.packages("ggplot2")
loadPkg("ggplot2")
ggplot(knn_different_k,
aes(x = k, y = accuracy)) +
geom_line(color = "orange", size = 1.5) +
geom_point(size = 3) +
labs(title = "accuracy vs k")
v1=c(2,3,4,10,11,12,20,25,30) # use the example in the pptx, try k=2 clusters
for( cut in 2:(length(v1)-2) ) {
print(
paste(
"mean 1 = ",mean(v1[1:cut]),
"mean 2 = ", mean(v1[(cut+1):length(v1)]),
"Sum of Sqaures  = ", (cut-1)*var(v1[1:cut]) + (length(v1)-cut-1)*var(v1[(cut+1):length(v1)]),
"cluster 1 len = ", cut, "cluster 2 len = ", (length(v1) - cut)
)
)
}
v1=c(2,3,4,10,11,12,20,25,30)
v1clusters <- kmeans(v1,3)
v1clusters
v2 <- NULL
v2 <- as.data.frame(v1)
colnames(v2) <- c("var1")
v2$var2 <- c(30,21,26,15,8,11,5,9,6)
plot(v2[,1:2], main = "K-means, var2 vs var1")
v2
v2clusters3 <- kmeans(v2,3)
v2clusters3
v2$label = v2clusters3$cluster
xkabledplyhead(v2, rows = 2)
xkabledplytail(v2, rows = 2)
plot(v2$var1,v2$var2, col=v2$label, main = "3 clusters: var2 vs var1", xlab = "var1", ylab = "var2")
ggplot(v2, aes(x=var1, y=var2, color= factor(label) )) + geom_point() + labs( title = "3 clusters: var2 vs var1", color="Cluster" )
v2clusters2 <- kmeans(v2,2)
#v2clusters2
v2$label = v2clusters2$cluster
xkabledplyhead(v2, rows = 2)
xkabledplytail(v2, rows = 2)
plot(v2$var1,v2$var2, col=v2$label, main = "2 clusters: var2 vs var1", xlab = "var1", ylab = "var2")
ggplot(v2, aes(x=var1, y=var2, color= factor(label) )) + geom_point() + labs( title = "2 clusters: var2 vs var1", color="Cluster" )
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose)
predlogitrose <- predict(logitrose, newdata=test1[-11], type="response")
confusion <- table(test1$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrise, test1[-11], type = "response")
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose)
predlogitrose <- predict(logitrose, newdata=test1[-11], type="response")
confusion <- table(test1$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1[-11], type = "response")
test_roc = roc(test1$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose1)
predlogitrose <- predict(logitrose, newdata=test1_fe[-7], type="response")
confusion <- table(test1_fe$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1_fe[-11], type = "response")
test_roc = roc(test1_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
fit <- rpart(stroke~ . , data = trainrose , method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, test1[-11], type = 'class')
confusion <- table(test1$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(predict_unseen, test1[-11], type = "response")
predict_unseen <-predict(fit, test1[-11], type = 'class')
confusion <- table(test1$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
FPR = FP/(FP+TN)
TPR = (TP)/(TP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
plot(FPR,TPR,main = "ROC Curve",col = 2,lwd = 2,type = "l",xlab = "False Positive Rate", ylab = "True positive Rate")
predict_unseen <-predict(fit, test1[-11], type = 'class')
confusion <- table(test1$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
FPR = FP/(FP+TN)
TPR = (TP)/(TP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
plot(FPR,TPR,main = "ROC Curve",col = 2,lwd = 2,type = "l",xlab = "False Positive Rate", ylab = "True positive Rate")
abline(a = 0,b = 1,lwd = 2,lty = 3,col = "black")
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose1)
predlogitrose <- predict(logitrose, newdata=test1_fe[-7], type="response")
confusion <- table(test1_fe$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1_fe[-11], type = "response")
test_roc = roc(test1_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
plot(test_roc)
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose1)
predlogitrose <- predict(logitrose, newdata=test1_fe[-7], type="response")
confusion <- table(test1_fe$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1_fe[-11], type = "response")
test_roc = roc(test1_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=train)
predlogitrose <- predict(logitrose, newdata=test[-11], type="response")
confusion <- table(test$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test[-11], type = "response")
test_roc = roc(test$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=train_fe)
predlogitrose <- predict(logitrose, newdata=test_fe[-7], type="response")
confusion <- table(test_fe$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test_fe[-7], type = "response")
test_roc = roc(test_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test[-11])
# Confusion Matrix
confusion = table(test$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImpPlot(classifier_RF)
