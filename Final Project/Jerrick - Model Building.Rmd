---
title: "Brain Stroke Prediction"
author: "Team Sage"
date: "`r Sys.Date()`"
output:  
    rmdformats::readthedown:
      toc_float: true
      number_sections: true
      
---


```{r setup, include=FALSE}

knitr::opts_chunk$set(echo = FALSE)
library(ggplot2)
library(gridExtra)
library(corrplot)
library(ROSE)
library(dplyr)
library(CatEncoders)

```

## 2 Viewing the dataset

```{r}
stroke_df = read.csv("healthcare-dataset-stroke-data.csv")


head(stroke_df)
# Getting the summary of the dataframe
summary(stroke_df)
# Looking at the structure of the Dataframe
str(stroke_df)
```

## 3 Converting the data to categorical and numerical respectively

```{r}

stroke_df = subset(stroke_df, select = -c(id))
# converting the numeric variables to factor variables
stroke_df$gender = as.factor(stroke_df$gender)
stroke_df$hypertension = as.factor(stroke_df$hypertension)
stroke_df$heart_disease = as.factor(stroke_df$heart_disease)
stroke_df$ever_married = as.factor(stroke_df$ever_married)
stroke_df$bmi = as.numeric(stroke_df$bmi)
stroke_df$stroke = as.factor(stroke_df$stroke)
stroke_df$work_type = as.factor(stroke_df$work_type)
stroke_df$Residence_type = as.factor(stroke_df$Residence_type)
stroke_df$smoking_status = as.factor(stroke_df$smoking_status)
# To get the summary statistics of the dataset
summary(stroke_df)
```
## 4 Dealing with NA values

```{r}
# There are 201 NA's in BMI. Replacing those variables with mean of BMI
stroke_df$bmi[is.na(stroke_df$bmi)] = mean(stroke_df$bmi,na.rm = TRUE)
# Finding if there are any NA's after imputation of null values
paste("The NA's in the dataset is:",sum(is.na(stroke_df)))
```






```{r}


ind <- sample(2, nrow(stroke_df), replace = TRUE, prob = c(0.7, 0.3))
train <- stroke_df[ind==1,]
test <- stroke_df[ind==2,]
dim(train)
dim(test)

```




```{r}

logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=train)
predlogitrose <- predict(logitrose, newdata=test[-11], type="response")
confusion <- table(test$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))




```


```{r}


fit <- rpart(stroke~ ., data = train , method = 'class')
rpart.plot(fit, extra = 106)
```



```{r}
predict_unseen <-predict(fit, test[-11], type = 'class')
confusion <- table(test$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))







```



```{r}


control <- rpart.control(minsplit = 5,
    minbucket = round(5 / 3),
    maxdepth = 6,
    cp = 0)
tune_fit <- rpart(stroke~., data = train , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
```

```{r}
predict_unseen1 <-predict(tune_fit, test[-11], type = 'class')
confusion <- table(test$stroke, predict_unseen1)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))






```






```{r}

classifier_RF = randomForest(x = train[-11],
                             y = train$stroke,
                             ntree = 500)
  
classifier_RF
```


```{r}
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test[-11])
# Confusion Matrix
confusion = table(test$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))





```












