---
title: "Brain Stroke Prediction"
author: "Team Sage"
date: "`r Sys.Date()`"
output:  
    rmdformats::readthedown:
      toc_float: true
      number_sections: true
      
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
options(scipen = 999,  digits = 3, big.mark=",", warn = -1)
```

```{r basicfunct, include=FALSE}
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

# Chapter 1: Introduction
## Why Brain Stroke Prediction ? Prior research and analysis.
A brain attack, also known as a stroke, it happens when something prevents the blood flow to a certain area of the brain or when a blood artery in the brain leaks. In 2020, stroke was the cause of 1 in 6 fatalities from cardiovascular disease. A stroke occurs in the United States every 40 seconds. One person has a stroke and dies every 3.5 minutes. In the US, there are more than 795,000 stroke victims annually. Of these, about 610,000 are new or first strokes. Nearly 1 in 4 strokes, or about 185,000, occur among persons who have already experienced a stroke. Ischemic strokes account for around 87% of all strokes Stroke is one of the leading causes of death and disability in the USA.Anyone can have a stroke, regardless of their age, gender, or background. According to the US Department of Health and Human Services' National Institutes of Health (NIH), 795,000 people in the US have strokes annually, and 137,000 of them will die.  According to the World Health Organization (WHO), stroke is the second most common cause of death worldwide, accounting for roughly 11% of all fatalities.

Ref:[link](https://www.nichd.nih.gov/health/topics/stroke/conditioninfo/risk)

# Chapter 2: Description of the Data
## Source of the Data
Presently, our dataset has a total of 5,110 observations across 12 variables. (See below for a readout of the dataset's structure and variable names.) Variable descriptions and additional information on the dataset comes from the following [Link.](https://www.kaggle.com/datasets/fedesoriano/stroke-prediction-dataset)

```{r basic_libraries, include=FALSE}
library(ggplot2)
library(gridExtra)
library(corrplot)
library(ROSE)
library(dplyr)
library(smotefamily)
library(caTools)
library(randomForest)
```

```{r load_csv, include=FALSE}
stroke_df = read.csv("healthcare-dataset-stroke-data.csv")
```

```{r desc, include=TRUE}
str(stroke_df)
```

- id (a unique identification number for each observation)
- gender (3 categories "Male", "Female", or "Other")
- age (age of the individual)
- hypertension (blood pressure of an individual)
- heart_disease (history of any heart disease previously noted)
- ever_married (2 categories "No" or "Yes")
- work_type (5 categories "children", "govt_job", "never_worked", "private", "self-employed")
- residence_type (where does the individual reside "rural" or "urban")
- avg_glucose_level (Average Blood Sugar level of an individual)
- bmi (Body Mass Index of the individual)
- smoking_status (smoking status of an individual)
- stroke (Target variable "1" or "0") 

## Data Manipulation and Cleaning 

- Converting the character value columns to categorical values and numerical values based on the type of the column.
- Checking for Null values in the dataset
- Null value imputation with mean
- subsetting the data into people who got stroke and the ones that didnt 
- subsetting the data into male and females who got stroke 
- created a separate categorical column for bmi using the bucketing system given in [Wikipedia](https://en.wikipedia.org/wiki/Body_mass_index)
- created a separate categorical column for average glucose level and age

For our exploratory data analysis, we ignored id because this is an independent variable and has no relationship with stroke. Following this we cleaned the dataset in which we removed the id.

```{r data_cleaning, include=FALSE}
stroke_df = subset(stroke_df, select = -c(id))

# converting the numeric variables to factor variables
stroke_df$gender = as.factor(stroke_df$gender)
stroke_df$hypertension = as.factor(stroke_df$hypertension)
stroke_df$heart_disease = as.factor(stroke_df$heart_disease)
stroke_df$ever_married = as.factor(stroke_df$ever_married)
stroke_df$bmi = as.numeric(stroke_df$bmi)
stroke_df$stroke = as.factor(stroke_df$stroke)
stroke_df$work_type = as.factor(stroke_df$work_type)
stroke_df$Residence_type = as.factor(stroke_df$Residence_type)
stroke_df$smoking_status = as.factor(stroke_df$smoking_status)
# To get the summary statistics of the dataset
summary(stroke_df)

```

```{r find_na_fix, include=FALSE}
# To find the NA's in the dataset
paste("The NA's in the dataset is:",sum(is.na(stroke_df)))

```


```{r na_fix, include=FALSE}

# Replacing NA values with average BMI value
stroke_df$bmi[is.na(stroke_df$bmi)] = mean(stroke_df$bmi,na.rm = TRUE)
paste("The NA's in the dataset after imputation of BMI with mean is:",sum(is.na(stroke_df)))
```

```{r summary_after_cleaning, include=TRUE}
# since we had only 1 data point in others category for gender we are removing it
stroke_df = subset(stroke_df,gender!="Other")
summary(stroke_df)
```

```{r subsets, include=FALSE}
# subsetting the data for various analyses 
stroke_1 = subset(stroke_df, stroke == 1)
stroke_0 = subset(stroke_df, stroke == 0)
stroke_1_female = subset(stroke_df, stroke == 1 & gender == "Female")
stroke_1_male = subset(stroke_df, stroke == 1 & gender == "Male")

# creating different columns for bmi, age and average_glucose_level based on different bucketing for each variables
dat <- within(stroke_df, {   
  bmi.cat = NA # need to initialize variable
  bmi.cat[bmi < 18.5] = "underweight"
  bmi.cat[bmi >= 18.5 & bmi < 25] = "normal"
  bmi.cat[bmi >= 25 & bmi < 30] = "overweight"
  bmi.cat[bmi >= 30 & bmi < 40] = "obesity"
  bmi.cat[bmi >=40] = "severe obesity" 
  avg_gluc.cat = NA # need to initialize variable
  avg_gluc.cat[avg_glucose_level < 60] = "Below 60"
  avg_gluc.cat[avg_glucose_level >= 60 & avg_glucose_level < 90] = "60 - 90"
  avg_gluc.cat[avg_glucose_level >= 90 & avg_glucose_level < 120] = "90 - 120"
  avg_gluc.cat[avg_glucose_level >= 120 & avg_glucose_level < 180] = "120 - 180"
  avg_gluc.cat[avg_glucose_level >= 180 & avg_glucose_level < 273] = "180 - 273"
  age.cat = NA
  age.cat[age<=20] = "Under 20"
  age.cat[age>=21 & age<= 40 ] = "20-40"
  age.cat[age>=41 & age<= 60 ] = "40-60"
  age.cat[age>=61 & age<= 80 ] = "60-80"
  age.cat[age>=80 ] = "above 80"
   } )

dat$bmi.cat<- factor(dat$bmi.cat,levels= c("underweight","normal","overweight","obesity","severe obesity" ))
dat$avg_gluc.cat<- factor(dat$avg_gluc.cat,levels= c("Below 60","60 - 90","90 - 120","120 - 180","180 - 273" ))
dat$age.cat<- factor(dat$age.cat,levels= c("Under 20","20-40","40-60","60-80","above 80"))
```


```{r}
#Smote_data <- ubBalance(predictor_variables, 
 #                       response_variable, 
  #                      type='ubSMOTE',     # Option for SMOTE
   #                     k = 3,              # How many neighbouring data points to consider
    #                    percOver = 57787.6, # Percentage of minority cases 284315/492 *100
    #                    percUnder = 100,    # Percentage of majority cases default 100
      #                  verbose = TRUE)
```
```{r}
set.seed(1234)
trainIndex = sample(1:nrow(dat), size=round(0.75*nrow(dat)), replace=FALSE)
train <- dat[trainIndex,]
test  <- dat[-trainIndex,]

dim(train)
dim(test)
```
```{r}
str(train[0:11])
```

```{r}
set.seed(1234)

trainrose<-ROSE(stroke~.,data=train[0:11])$data

ggplot(trainrose, aes(x=stroke,fill=stroke))+geom_bar()+ggtitle("Distribution of Target varibale (Stroke)")+ theme_bw() +  theme()+ xlab("Stroke") + ylab("Count of people")
```
```{r}
table(train$stroke)
table(trainrose$stroke)
```

## LOGIT MODEL

```{r}
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose)
testrose_x <- test
predlogitrose <- predict(logitrose, newdata=test, type="response")
confusion <- table(test$stroke, predlogitrose >= 0.5)
confusion
#predlogitrose <- ifelse(predlogitrose>0.5, 1, 0)
#ClassificationError <- mean(predlogitrose != test$stroke) 
#print(paste("Accuracy = ", 1-ClassificationError)) 

TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(FP)/(FP+TN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
False_positive_rate =(FP)/(FP+TN)
False_negative_rate =(FN)/(FN+TP)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('False_positive_rate', False_positive_rate))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
```
## DECISION TREE

```{r}
library(rpart)
library(rpart.plot)
fit <- rpart(stroke~ ., data = trainrose , method = 'class')
rpart.plot(fit, extra = 106)
```
```{r}
predict_unseen <-predict(fit, test, type = 'class')
confusion1 <- table(test$stroke, predict_unseen)
confusion1

TN =confusion1[1,1]
TP =confusion1[2,2]
FP =confusion1[1,2]
FN =confusion1[2,1]
precision =(TP)/(TP+FP)
recall_score =(FP)/(FP+TN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
False_positive_rate =(FP)/(FP+TN)
False_negative_rate =(FN)/(FN+TP)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('False_positive_rate', False_positive_rate))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
```

```{r}
control <- rpart.control(minsplit = 5,
    minbucket = round(5 / 3),
    maxdepth = 6,
    cp = 0)
tune_fit <- rpart(stroke~., data = trainrose , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
predict_unseen1 <-predict(tune_fit, test, type = 'class')
table_mat <- table(test$stroke, predict_unseen1)
table_mat
TN =table_mat[1,1]
TP =table_mat[2,2]
FP =table_mat[1,2]
FN =table_mat[2,1]
precision =(TP)/(TP+FP)
recall_score =(FP)/(FP+TN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
False_positive_rate =(FP)/(FP+TN)
False_negative_rate =(FN)/(FN+TP)
accuracy_Test <- sum(diag(table_mat)) / sum(table_mat)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('False_positive_rate', False_positive_rate))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
```
```{r}
trainrose[-11]
```

```{r}
classifier_RF = randomForest(x = trainrose[-11],
                             y = trainrose$stroke,
                             ntree = 500)
  
classifier_RF
```

```{r}
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test[-11])

# Confusion Matrix
confusion_mtx = table(test$stroke, y_pred)
confusion_mtx
TN =confusion_mtx[1,1]
TP =confusion_mtx[2,2]
FP =confusion_mtx[1,2]
FN =confusion_mtx[2,1]
precision =(TP)/(TP+FP)
recall_score =(FP)/(FP+TN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
False_positive_rate =(FP)/(FP+TN)
False_negative_rate =(FN)/(FN+TP)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('False_positive_rate', False_positive_rate))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
```
