chisq.test(stroke_df$Residence_type, stroke_df$stroke)
set.seed(80)
# In the train data we are using under and oversampling to address the class imbalance problem
ind <- sample(2, nrow(stroke_df), replace = TRUE, prob = c(0.7, 0.3))
train <- stroke_df[ind==1,]
test <- stroke_df[ind==2,]
dim(train)
dim(test)
train <- ovun.sample(stroke~., data=train, method = "both",p = 0.5,seed = 222)$data
r<-table(train$stroke)
paste("Seeing if the class imabalance is fixed")
r
str(test)
ggplot(train, aes(x=stroke,fill=stroke))+geom_bar()+ggtitle("Distribution of Target variable (Stroke) after balancing using ")+ theme_bw() + theme()+ xlab("Stroke") + ylab("Count of people")
str(train)
set.seed(123)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=train)
predlogitrose <- predict(logitrose, newdata=test[-11], type="response")
confusion <- table(test$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test[-11], type = "response")
test_roc = roc(test$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
library(ROSE)
set.seed(1234)
train_fe = subset(train, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
test_fe = subset(test, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
str(train_fe[-7])
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=train_fe)
predlogitrose <- predict(logitrose, newdata=test_fe[-7], type="response")
confusion <- table(test_fe$stroke, predlogitrose >= 0.5)
confusion
#plot(density(predlogitrose))
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test_fe[-7], type = "response")
test_roc = roc(test_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
fit <- rpart(stroke~ ., data = train , method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, test[-11], type = 'class')
confusion <- table(test$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
fit <- rpart(stroke~ ., data = train_fe , method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, test_fe[-7], type = 'class')
confusion <- table(test_fe$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
control <- rpart.control(minsplit = 5,
minbucket = round(5 / 3),
maxdepth = 6,
cp = 0)
tune_fit <- rpart(stroke~., data = train , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
predict_unseen1 <-predict(tune_fit, test[-11], type = 'class')
confusion <- table(test$stroke, predict_unseen1)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
control <- rpart.control(minsplit = 5,
minbucket = round(5 / 3),
maxdepth = 6,
cp = 0)
tune_fit <- rpart(stroke~., data = train_fe , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
predict_unseen1 <-predict(tune_fit, test_fe[-7], type = 'class')
confusion <- table(test_fe$stroke, predict_unseen1)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
classifier_RF = randomForest(x = train[-11],
y = train$stroke,
ntree = 500)
classifier_RF
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test[-11])
# Confusion Matrix
confusion = table(test$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImpPlot(classifier_RF)
train_fe1 = subset(train, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
test_fe1 = subset(test, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
dim(train_fe1)
dim(test_fe1)
classifier_RF = randomForest(x = train_fe1[-7],
y = train_fe1$stroke,
ntree = 500)
classifier_RF
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImp(classifier_RF)
classifier_RF1 = randomForest(x = train_fe[-7],
y = train_fe$stroke,
ntree = 500)
classifier_RF1
# Predicting the Test set results
y_pred = predict(classifier_RF1, newdata = test_fe[-7])
# Confusion Matrix
confusion = table(test_fe$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
set.seed(1234)
trainIndex = sample(1:nrow(stroke_df), size=round(0.75*nrow(stroke_df)), replace=FALSE)
train1 <- stroke_df[trainIndex,]
test1  <- stroke_df[-trainIndex,]
dim(train1)
dim(test1)
library(ROSE)
set.seed(1234)
train_fe = subset(train1, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
test1_fe = subset(test1, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
str(train_fe)
dim(test1_fe)
set.seed(1234)
trainrose1<-ROSE(stroke~.,data=train_fe)$data
ggplot(trainrose1, aes(x=stroke,fill=stroke))+geom_bar()+ggtitle("Distribution of Target varibale (Stroke)")+ theme_bw() +  theme()+ xlab("Stroke") + ylab("Count of people")
table(trainrose1$stroke)
set.seed(1234)
trainrose<-ROSE(stroke~.,data=train1)$data
ggplot(trainrose, aes(x=stroke,fill=stroke))+geom_bar()+ggtitle("Distribution of Target varibale (Stroke)")+ theme_bw() +  theme()+ xlab("Stroke") + ylab("Count of people")
trainrose$age = abs(trainrose$age)
trainrose$avg_glucose_level = abs(trainrose$avg_glucose_level)
summary(trainrose)
trainrose1$age = abs(trainrose1$age)
trainrose1$avg_glucose_level = abs(trainrose1$avg_glucose_level)
summary(trainrose1)
table(train_fe$stroke)
table(trainrose1$stroke)
table(test1_fe$stroke)
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose)
predlogitrose <- predict(logitrose, newdata=test1[-11], type="response")
confusion <- table(test1$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1[-11], type = "response")
test_roc = roc(test1$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
test_roc
set.seed(1234)
logitrose  <- glm(stroke ~., family=binomial (link="logit"), data=trainrose1)
predlogitrose <- predict(logitrose, newdata=test1_fe[-7], type="response")
confusion <- table(test1_fe$stroke, predlogitrose >= 0.5)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
test_prob = predict(logitrose, test1_fe[-11], type = "response")
test_roc = roc(test1_fe$stroke ~ test_prob, plot = TRUE, print.auc = TRUE)
fit <- rpart(stroke~ . , data = trainrose , method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, test1[-11], type = 'class')
confusion <- table(test1$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
FPR = FP/(FP+TN)
TPR = (TP)/(TP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
fit <- rpart(stroke~ . , data = trainrose1 , method = 'class')
rpart.plot(fit, extra = 106)
predict_unseen <-predict(fit, test1_fe[-7], type = 'class')
confusion <- table(test1_fe$stroke, predict_unseen)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
control <- rpart.control(minsplit = 7,
minbucket = round(5 / 3),
maxdepth = 4,
cp = 0)
tune_fit <- rpart(stroke~., data = trainrose , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
predict_unseen1 <-predict(tune_fit, test1, type = 'class')
confusion <- table(test1$stroke, predict_unseen1)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
control <- rpart.control(minsplit = 7,
minbucket = round(5 / 3),
maxdepth = 4,
cp = 0)
tune_fit <- rpart(stroke~., data = trainrose1 , method = 'class', control = control)
rpart.plot(tune_fit, extra = 106)
predict_unseen1 <-predict(tune_fit, test1_fe, type = 'class')
confusion <- table(test1_fe$stroke, predict_unseen1)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
classifier_RF = randomForest(x = trainrose[-11],
y = trainrose$stroke,
ntree = 500)
classifier_RF
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test1[-11])
# Confusion Matrix
confusion = table(test1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
classifier_RF = randomForest(x = trainrose1[-7],
y = trainrose1$stroke,
ntree = 500)
classifier_RF
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test1_fe[-7])
# Confusion Matrix
confusion = table(test1_fe$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
train_fe1 = subset(train, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
test_fe1 = subset(test, select = c(age,avg_glucose_level,work_type,hypertension,smoking_status,bmi,stroke))
dim(train_fe1)
dim(test_fe1)
classifier_RF = randomForest(x = train_fe1[-7],
y = train_fe1$stroke,
ntree = 500,importance = TRUE)
classifier_RF
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImp(classifier_RF)
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImp(classifier_RF$importance)
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
classifier_RF$importance
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
varImp(classifier_RF)
# Predicting the Test set results
y_pred = predict(classifier_RF, newdata = test_fe1[-7])
# Confusion Matrix
confusion = table(test_fe1$stroke, y_pred)
confusion
# Eval Metric
TN =confusion[1,1]
TP =confusion[2,2]
FP =confusion[1,2]
FN =confusion[2,1]
precision =(TP)/(TP+FP)
recall_score =(TP)/(TP+FN)
f1_score=2*((precision*recall_score)/(precision+recall_score))
accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
print(paste('Accuracy for test', accuracy_model))
print(paste('precision for test', precision))
print(paste('f1_score for test', f1_score))
print(paste('recall_score for test', recall_score))
classifier_RF$importance
